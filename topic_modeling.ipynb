{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 필요 패키지 설치\n",
    "# !pip install bertopic\n",
    "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from eunjeon import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "from pykospacing import Spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/real_data'\n",
    "\n",
    "file_paths = glob.glob(os.path.join(data_dir, '*.xlsx'))\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for file in file_paths:\n",
    "      # 데이터 불러오기\n",
    "      data = pd.read_excel(file)\n",
    "      # 데이터프레임 합치기\n",
    "      dataset = pd.concat([dataset, data], ignore_index=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 리뷰 삭제\n",
    "def remove_duplicates(dataset):\n",
    "\n",
    "  print(f\"중복 제거 전 데이터 크기 : {len(dataset)}\")\n",
    "  dataset.drop_duplicates(subset=['리뷰'], inplace=True) \n",
    "  print(f\"중복 제거 후 전체 데이터 크기 : {len(dataset)}\")\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 제외한 문자 제거\n",
    "def remove_not_korean(dataset):\n",
    "  review_removed = list(map(lambda review: re.sub('[^가-힣 ]', '', review), dataset['리뷰']))\n",
    "  dataset['리뷰'] = review_removed\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = remove_not_korean(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 변환기\n",
    "def process_spacing(dataset):\n",
    "  spacing = Spacing() \n",
    "  spacing_review = list()\n",
    "  for review in tqdm.tqdm(dataset['리뷰']):\n",
    "    spacing_review.append(spacing(review))\n",
    "  \n",
    "  dataset['리뷰'] = spacing_review\n",
    "\n",
    "  return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_spacing(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정적 리뷰(평점 5점 만점 기준 4, 5점)\n",
    "review_pos = dataset[(dataset['별점'] == 4) | (dataset['별점'] == 5)]['리뷰']\n",
    "# 부정적 리뷰(평점 5점 만점 기준 1, 2, 3점)\n",
    "review_neg = dataset[(dataset['별점'] == 1) | (dataset['별점'] == 2) | (dataset['리뷰'] == 3)]['리뷰']\n",
    "\n",
    "review_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pos = pd.DataFrame(review_pos, columns=['리뷰'])\n",
    "review_neg = pd.DataFrame(review_neg, columns=['리뷰'])\n",
    "\n",
    "review_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"긍정적 리뷰 개수 : {len(review_pos)}\")\n",
    "print(f\"부정적 리뷰 개수 : {len(review_neg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#치환 리스트\n",
    "replace_list = pd.read_excel('data/preprocess_list/replace_list.xlsx')\n",
    "replace_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 치환 리스트 적용\n",
    "def replace_word(dataset):\n",
    "    review_replaced_list = []\n",
    "    for review in tqdm.tqdm(dataset['리뷰']):\n",
    "        review = ''.join(review)  # 리스트의 요소들을 공백으로 연결\n",
    "        review = str(review)\n",
    "        for before, after in zip(replace_list['before_replacement'], replace_list['after_replacement']):\n",
    "            review = review.replace(before, after)  # 각 치환 적용\n",
    "        review_replaced_list.append(review)  # 최종적으로 치환된 리뷰를 리스트에 추가\n",
    "\n",
    "    dataset['치환된 리뷰'] = review_replaced_list\n",
    "    \n",
    "    len(dataset['리뷰'])\n",
    "    len(review_replaced_list)\n",
    "    \n",
    "    dataset['치환된 리뷰'] = review_replaced_list\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pos = replace_word(review_pos)\n",
    "review_neg = replace_word(review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pos.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(dataset):\n",
    "    result = []\n",
    "    for review in dataset['치환된 리뷰']:\n",
    "        #nouns : 명사 추출, pos : 품사 부착, morphs : 형태소 추출\n",
    "        result.append(mecab.nouns(str(review)))\n",
    "\n",
    "    print(len(dataset['치환된 리뷰']))\n",
    "    print(len(result))\n",
    "    dataset['토큰'] = result\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "mecab = Mecab()\n",
    "\n",
    "review_tokenized_pos = tokenizer(review_pos)\n",
    "review_tokenized_neg = tokenizer(review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_tokenized_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'긍정적 리뷰 형태소 개수 : {sum(len(sublist) for sublist in review_tokenized_pos[\"토큰\"])}')\n",
    "print(f'긍정적 리뷰 형태소 개수 : {sum(len(sublist) for sublist in review_tokenized_neg[\"토큰\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 리스트 정의\n",
    "stopword_list = pd.read_excel('data/preprocess_list/stopword_list.xlsx')\n",
    "stopword_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 글자 토픽 정의\n",
    "one_char_keyword = pd.read_excel('data/preprocess_list/one_char_list.xlsx')\n",
    "one_char_keyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 욕설 리스트 정의\n",
    "abuse_list = pd.read_excel('data/preprocess_list/fword_list.xlsx')\n",
    "abuse_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 및 한 글자 토픽 제외한 한 글자 단어 삭제\n",
    "def remove_stopword(dataset):\n",
    "    review_removed_stopword = []\n",
    "    for tokens in dataset['토큰']:\n",
    "        token_removed_stopword = []\n",
    "        for token in tokens:\n",
    "            # 토큰이 욕설 리스트에 없는 경우\n",
    "            if token not in list(abuse_list['fword']):\n",
    "                # 토큰의 글자 수가 2글자 이상인 경우\n",
    "                if 1 < len(token):\n",
    "                    # 토큰이 불용어가 아닌 경우만 분석용 리뷰 데이터로 포함\n",
    "                    if token not in list(stopword_list['stopword']):\n",
    "                        token_removed_stopword.append(token)\n",
    "                # 토큰의 글자 수가 1글자인 경우\n",
    "                else:\n",
    "                    # 1글자 키워드에 포함되는 경우만 분석용 리뷰 데이터로 포함\n",
    "                    if token in list(one_char_keyword['one_char_keyword']):\n",
    "                        token_removed_stopword.append(token)\n",
    "            \n",
    "        review_removed_stopword.append(token_removed_stopword)\n",
    "    \n",
    "    dataset['불용어 제거 후 토큰'] = review_removed_stopword\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정 리뷰, 부정 리뷰 각각에 욕설 제거, 불용어 제거, 한 글자 키워드 제외 한 글자 단어 제거 적용\n",
    "review_removed_stopword_pos = remove_stopword(review_tokenized_pos)\n",
    "review_removed_stopword_neg = remove_stopword(review_tokenized_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_removed_stopword_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'단어 필터링 후 긍정적 리뷰 형태소 개수 : {sum(len(sublist) for sublist in review_removed_stopword_pos[\"불용어 제거 후 토큰\"])}')\n",
    "print(f'단어 필터링 후 부정적 리뷰 형태소 개수 : {sum(len(sublist) for sublist in review_removed_stopword_neg[\"불용어 제거 후 토큰\"])}')\n",
    "print(len(review_removed_stopword_pos[\"불용어 제거 후 토큰\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거 후 토큰이 없는 경우, 해당 데이터 행 제거\n",
    "review_removed_stopword_pos = review_removed_stopword_pos[review_removed_stopword_pos['불용어 제거 후 토큰'] != '[]']\n",
    "review_removed_stopword_neg = review_removed_stopword_neg[review_removed_stopword_neg['불용어 제거 후 토큰'] != '[]']\n",
    "\n",
    "review_removed_stopword_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 문자열로 결합\n",
    "review_removed_stopword_pos[\"학습용 데이터\"] = [' '.join(tokens) for tokens in review_removed_stopword_pos[\"불용어 제거 후 토큰\"]]\n",
    "review_removed_stopword_neg[\"학습용 데이터\"] = [' '.join(tokens) for tokens in review_removed_stopword_neg[\"불용어 제거 후 토큰\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_removed_stopword_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 후 데이터를 excel 파일로 export\n",
    "df1 = pd.DataFrame(review_removed_stopword_pos, columns=['리뷰','치환된 리뷰','토큰','불용어 제거 후 토큰','학습용 데이터'])\n",
    "df2 = pd.DataFrame(review_removed_stopword_neg, columns=['리뷰','치환된 리뷰','토큰','불용어 제거 후 토큰','학습용 데이터'])\n",
    "\n",
    "df1.to_excel(\"data/preprocessed_data/processed_pos_data.xlsx\", index = False, engine='openpyxl')\n",
    "df2.to_excel(\"data/preprocessed_data/processed_neg_data.xlsx\", index = False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 후 데이터 load\n",
    "review_removed_stopword_pos = pd.read_excel('data/preprocessed_data/processed_pos_data.xlsx')\n",
    "review_removed_stopword_neg = pd.read_excel('data/preprocessed_data/processed_neg_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터에 null 있는지 체크\n",
    "print(review_removed_stopword_pos['학습용 데이터'].isnull().sum())\n",
    "print(review_removed_stopword_neg['학습용 데이터'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# 한국어 BERT 임베딩 모델 로드\n",
    "embedding_model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')\n",
    "umap_model = umap.UMAP(n_neighbors = 10, min_dist = 0.1, n_components=4, random_state=42, metric='euclidean')\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size = 6, metric = 'euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "representation_model = KeyBERTInspired()\n",
    "zeroshot_topic_list = ['스토리','힐링','재미','캐릭터','더빙','시스템','컨텐츠']\n",
    "\n",
    "hyperparams = {\n",
    "     'top_n_words' : 10,\n",
    "     'nr_topics' : 20,\n",
    "     'n_gram_range' : (1, 1),\n",
    "     'min_topic_size' : 10,\n",
    "     'calculate_probabilities' : True,\n",
    "     'zeroshot_topic_list' : zeroshot_topic_list,\n",
    "     'zeroshot_min_similarity' : 85,\n",
    "}\n",
    "\n",
    "# BERTopic 모델 생성 및 학습\n",
    "topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, representation_model=representation_model, **hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic 모델 적용\n",
    "pos_topics, pos_probabilities = topic_model.fit_transform(review_removed_stopword_pos['학습용 데이터'])\n",
    "neg_topics, neg_probabilities = topic_model.fit_transform(review_removed_stopword_neg['학습용 데이터'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 리뷰 + 토픽 저장\n",
    "review_removed_stopword_pos['토픽'] = pos_topics\n",
    "pos_max_probabilities = np.max(pos_probabilities, axis=1)\n",
    "review_removed_stopword_pos['토픽에 속할 확률'] = pos_max_probabilities\n",
    "\n",
    "review_removed_stopword_neg['토픽'] = neg_topics\n",
    "neg_max_probabilities = np.max(neg_probabilities, axis=1)\n",
    "review_removed_stopword_neg['토픽에 속할 확률'] = neg_max_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 매개변수 저장\n",
    "model_name = 'v4'\n",
    "review_removed_stopword_pos.to_excel(\"/result/result/after_train_data.xlsx\", index = False, engine='openpyxl')\n",
    "\n",
    "with open(f\"result/params/{model_name}.json\", \"w\") as f:\n",
    "    json.dump(hyperparams, f)\n",
    "print(f\"모델 매개변수가 {model_name}.json 파일로 저장되었습니다.\")\n",
    "\n",
    "topic_model.save(f\"result/models/model_{model_name}\")\n",
    "print(f\"모델이 {model_name}_model 파일로 저장되었습니다.\")\n",
    "\n",
    "topic_info_df = topic_model.get_topic_info()\n",
    "topic_info_df.to_excel(f'result/topics/{model_name}_topic_info.xlsx', index=False)\n",
    "print(f\"토픽 정보가 {model_name}_topic_info.xlsx 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roatg",
   "language": "python",
   "name": "roatg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
